{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#AD07FF\"> In this notebook you can see data cleaning, visualization, hypothesis testing,  <br> adding new features and preparing data so that we can run a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#FF7B07\"><div align=\"center\">**Table Of Contents**\n",
    "[<span style=\"color:#FF7B07\">**1. Introduction**](#1)<br>\n",
    "[<span style=\"color:#FF7B07\">**2. Import Libraries and Load Data**](#2)<br>\n",
    "[<span style=\"color:#FF7B07\">**3. Data Analysis**](#3)<br>\n",
    "[<span style=\"color:#FF7B07\">**4. Create Features**](#4)<br> \n",
    "[<span style=\"color:#FF7B07\">**5. Data Cleaning**](#5)<br>\n",
    "[<span style=\"color:#FF7B07\">**6. Data Preparation**](#6)<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#FF7B07\"><div align=\"center\">**Introduction** <a  name=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, a healthy lifestyle is becoming a valuable characteristic of a modern society. <br>\n",
    "More and more people try to enhance their health by doing regularly different sports and put emphasis on their food habits. <br>\n",
    "In order to satisfy the specific needs of every individual, conclusions gained out of the users’ data are from high importance. <br>\n",
    "Companies such as MyFitnessPal operate in the lucrative business field of health data. <br>\n",
    "The healthcare industry is booming, especially when it comes to the analysis of health-related data. <br>\n",
    "This report will show how the analysis of data will improve the life of the users, <br>\n",
    "but also highlight the potential for companies active in the business. <br>\n",
    "#### <span style=\"color:#FF7B07\">  **Source** https://www.kaggle.com/vetrirah/customer?select=Train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#FF7B07\"><div align=\"center\">**Import Libraries and Load Data** <a  name=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import preprocessing\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is large and this command will help us to see the whole result \n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/myFitnessPal_parsed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('foods.json') as json_file:\n",
    "     food_names = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/FoodWithAdditionalNutritionalInformation.json') as json_file:\n",
    "    foods = json.load(json_file)\n",
    "\n",
    "foods_and_groups = pd.read_excel('../data/MyFoodData-Nutrition-Facts-SpreadSheet-Release-1-4.xlsx')\n",
    "\n",
    "foods_and_groups = foods_and_groups.drop_duplicates(['name', 'Food Group'])[['name', 'Food Group']]\n",
    "foods_and_groups.columns = ['Food', 'Food Group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#FF7B07\"><div align=\"center\">**Data Analysis** <a  name=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\">Let us first review the general information of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n Data shape - {data.shape} ')\n",
    "print(f' Unique users - {data[\"user_id\"].nunique()} \\n')\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> As you can see most of the feature's type is number so we don't need to convert them <br>  except \"date\" and \"food_ids\" which needs to converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> At first glance we would probably see that data needs to be normalized and maybe we have some outliers too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> As we know we have a different number of records for each user in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'user_id 1 has {len(data[data[\"user_id\"]==1])} records')\n",
    "print(f'user_id 2 has {len(data[data[\"user_id\"]==2])} records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> User may also have missed information between the first and final days of the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"user_id\"]==7].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> As we can see, User whose ID is 7, made the first record in 2014-10-06 and the second in 2014-10-15. <br> We will need to reflect this information as new feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> Check for null values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = pd.DataFrame(data.isna().sum())\n",
    "null_df.columns = [\"Null Frequency\"]\n",
    "null_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> As we saw there are 10 columns which have sometimes null values <br> User can have a maximum of 10 columns with null value <br> So I think we should throw away a user who has 4 or more Null values, <br> Because otherwise it would turn out that we have filled more than 25 percent of the user information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Users which have null in more than 3 columns : ',(len(data.loc[data.isnull().sum(axis=1)>3])/len(data))*100,'%')\n",
    "nullColumns = data.loc[data.isnull().sum(axis=1)>3].index\n",
    "nullColumns = pd.DataFrame(nullColumns,columns={'index'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> It seems that the number of such people is about 4.6 percent. <br> Due to the fact that the data is not so little and at the same time filling 4 null feature is not so accurate we can throw them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> There can still be a problem with the values and such a situation is when the goal calories are 0 <br>and the mentioned situation does not make sense <br> because the main purpose of the app is to select a goal and get closer to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Users which have zero in goal_calories : ',(len(data[data['goal_calories']==0])/len(data))*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> It seems that the number of such people is about 0.1 percent. So we can throw them away "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPlot(data, col:str, ttext:str, xtext:str, ytext:str) ->None:\n",
    "    \"\"\"\n",
    "    this function creates histogram and boxplot by pyplot library and also count skewness\n",
    "    \n",
    "    Arguments:\n",
    "    data -- pandas dataframe\n",
    "    col -- column name which we want to plot\n",
    "    ttext -- string which we want to write on top of the graph\n",
    "    xtext -- string which we want to write on x axis\n",
    "    ytext -- string which we want to write on y axis\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    fig.add_trace(go.Histogram(x=data[col],marker_color='#6a6fff'),row=1,col=1)\n",
    "    fig.add_trace(go.Box(x=data[col]),row=1,col=2)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=ttext,\n",
    "        xaxis_title_text=xtext,\n",
    "        yaxis_title_text=ytext, \n",
    "        bargap=0.05, \n",
    "        template = 'plotly_dark',\n",
    "        width=900, height=600\n",
    "    )\n",
    "    fig.add_annotation(dict(font=dict(color='yellow',size=15),\n",
    "                                        x=0.35,\n",
    "                                        y=1.1,\n",
    "                                        showarrow=False,\n",
    "                                        text=\"Skewness = \"+str(data[col].skew()),\n",
    "                                        textangle=0,\n",
    "                                        xanchor='left',\n",
    "                                        xref=\"paper\",\n",
    "                                        yref=\"paper\"))\n",
    "    \n",
    "    fig.show()\n",
    "#createPlot(data,\"goal_calories\",\"Time Distribution\",\"Time (days)\",\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups = ['Low','Medium','High']\n",
    "# dfForPlot = df.copy();\n",
    "# def get_categories(x):\n",
    "#     if x < q1:\n",
    "#         return groups[0]\n",
    "#     elif x < q3:\n",
    "#         return groups[1]\n",
    "#     else:\n",
    "#         return groups[2]    \n",
    "\n",
    "# for col in numVariable:\n",
    "#     q1 = df[col].quantile(q=0.25)\n",
    "#     q3 = df[col].quantile(q=0.75)\n",
    "#     dfForPlot[col] = df[col].apply(get_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> There are many spreaded Hypotheses about diet, healthy lifestyle and also about habits too. Now let's check their validity with our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\">  1. People more likely start diet and healthy lifestyle on mondays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daysCount=[0,0,0,0,0,0,0]\n",
    "for ID in data['user_id'].unique():\n",
    "    date = data[data[\"user_id\"]==ID].head(1)['date'].values[0]\n",
    "    d=datetime.datetime.strptime(str(date), \"%Y-%m-%d\")\n",
    "    daysCount[d.weekday()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.dot([i / np.sum(daysCount) for i in daysCount],100)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "df=pd.DataFrame({'days':['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],'quantities': res})\n",
    "sns.barplot(x='days',y='quantities',data=df,palette='magma',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> The results show that hypothesis were right: <br>  people more likely start diet on mondays and the graph also proves that wednesday is really lazy day to start something. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\">  2. People frequently cheat diet on weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daysCount=[0,0,0,0,0,0,0]\n",
    "daysLogged=[0,0,0,0,0,0,0]\n",
    "for i,row in data.iterrows():\n",
    "    date = row['date']\n",
    "    goal_calories=row['goal_calories']\n",
    "    total_calories=row['total_calories']\n",
    "    if total_calories > (goal_calories+goal_calories*15) :  # actually total_calories more than approx. 5 hamburgers  😂 \n",
    "        d=datetime.datetime.strptime(str(date), \"%Y-%m-%d\")\n",
    "        daysCount[d.weekday()]+=1\n",
    "    daysLogged[d.weekday()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [i / j for i, j in zip(daysCount,daysLogged)]\n",
    "res =np.dot( [i / np.sum(res) for i in res],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "df=pd.DataFrame({'days':['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],'quantities': res})\n",
    "sns.barplot(x='days',y='quantities',data=df,palette='magma',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> Looks like people use to cheat on fridays and saturdays more, they took much more calories on these days. Our hypothesis partially justified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\">  3. 77 percent of users never use an app again 72 hours after installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_log=pd.DataFrame()\n",
    "apps_log[\"people\"] = data[\"user_id\"].value_counts()\n",
    "apps_log[\"days_used_app\"] = apps_log[\"people\"].apply(lambda x: len(apps_log[apps_log[\"people\"]==x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plott=sns.barplot(x='days_used_app',y='people',data=apps_log,palette='magma',ax=ax)\n",
    "plott=plott.set_xticklabels(plott.get_xticklabels(),rotation=45,ha=\"right\")\n",
    "# pltt.yticks(rotation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> Hypothesis is partly correct, app's usage reduces exponentially but It takes users almost 1 month to surrender. Maybe it's not the surrender, maybe it's the period they reach dreamy weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> Now look at the nutrients individually as taking extra nutrients can be dangerous For example taking too much sugar can cause diabetes <br> Let's draw total and goal nutrients together and see how many percentage is under or above risk <br> Based on this information we can tell the customer to reduce consumption or increase it depending on whether he is at risk or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nutrient(df,nutrient_name,goal_nutrient_name,thr1,thr2):\n",
    "    \"\"\"\n",
    "    This function creates 3 plots based on thresholds\n",
    "    \n",
    "    Arguments:\n",
    "    df -- pandas dataframe\n",
    "    nutrient_name -- column name which we want to plot\n",
    "    goal_nutrient_name -- column name which we want to plot\n",
    "    thr1 -- threshold on the values below which we draw the first drawing   \n",
    "    thr2 -- threshold on the values above which we draw the third drawing\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    avg = df.groupby(['user_id']).apply(lambda x: x[nutrient_name].sum()/len(x)).reset_index()\n",
    "    goal = df.groupby(['user_id']).apply(lambda x: x[goal_nutrient_name].sum()/len(x)).reset_index()\n",
    "    avg.columns = [\"user_id\", \"avg_nut\"]\n",
    "    goal.columns = [\"user_id\", \"goal_avg_nut\"]\n",
    "\n",
    "    fig, ax =plt.subplots(1,3,figsize=(17,7))\n",
    "    _max = max(df[nutrient_name].max(),df[goal_nutrient_name].max())\n",
    "    ax[0].hist(avg[avg['avg_nut']<thr1]['avg_nut'], range=[0,thr1], color='Red', fc='none', lw=2, histtype='step')\n",
    "    ax[0].hist(goal[goal['goal_avg_nut']<thr1]['goal_avg_nut'], range=[0,thr1], color='Green', fc='none', lw=2, histtype='step')\n",
    "    ax[0].legend(('Total', 'Goal'), loc=\"upper right\")\n",
    "    ax[1].hist(avg['avg_nut'], range=[thr1,thr2], color='Red',fc='none', lw=2, histtype='step')\n",
    "    ax[1].hist(goal['goal_avg_nut'], range = [thr1,thr2], color='Green', fc='none', lw=2, histtype='step')\n",
    "    ax[1].legend(('Total', 'Goal'), loc=\"upper right\")\n",
    "    ax[2].hist(avg[avg['avg_nut']>thr2]['avg_nut'], range=[thr2,_max], color='Red', fc='none', lw=2, histtype='step')\n",
    "    ax[2].hist(goal[goal['goal_avg_nut']>thr2]['goal_avg_nut'], range=[thr2,_max], color='Green', fc='none', lw=2, histtype='step')\n",
    "    ax[2].legend(('Total', 'Goal'),loc=\"upper right\")\n",
    "    \n",
    "    under_minimum = len(avg[avg['avg_nut'] < thr1])\n",
    "    above_minimum = len(avg[avg['avg_nut'] > thr2])\n",
    "    total = avg.user_id.nunique()\n",
    "\n",
    "    print(f'\\nlower risk group: {100*under_minimum/total} %')\n",
    "    print(f'above risk group: {100*above_minimum/total} %')\n",
    "    print(f'not   risk group: {100*(1-(under_minimum+above_minimum)/total)} %\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tenure = data['user_id'].value_counts().to_frame().reset_index()\n",
    "user_tenure.columns = ['user_id', 'Amount']\n",
    "\n",
    "top_users = pd.merge(data, user_tenure.head(1000), on='user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nutrient(top_users,\"total_sugar\",\"goal_sugar\",50,350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nutrient(top_users,\"total_carbs\",\"goal_carbs\",50,350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> Let's see what food they eat most often in month. <br> This information can be used by markets to determine how many products they should receive per month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elem_dict(all_names,lst):\n",
    "    for food_id in lst[1:-1].split(\",\"):\n",
    "        food_id = food_id.replace(' ','')\n",
    "        all_names[food_id] = all_names.get(food_id,0)+1\n",
    "        \n",
    "def get_top_food_ids(df,top_numbers):\n",
    "    all_names = {}\n",
    "    df[\"food_ids\"].apply(lambda lst: add_elem_dict(all_names,lst))\n",
    "    top_food_ids = list(sorted(all_names.items(), key=lambda item: item[1]))\n",
    "    top_food_ids = [key for key,value in top_food_ids]\n",
    "    top_food_ids.reverse()\n",
    "    return top_food_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"month\"] = data[\"date\"].apply(lambda x: x[5:7])\n",
    "max_food_in_month = data.groupby(['month']).apply(lambda x: get_top_food_ids(x,10))\n",
    "del data[\"month\"]\n",
    "\n",
    "top_food_dataframe = pd.DataFrame(max_food_in_month)\n",
    "top_food_dataframe = top_food_dataframe.apply(lambda x: x[0], axis = 1, result_type ='expand').reset_index()\n",
    "top_food_dataframe.columns = [\"month\",\"meal_1\",\"meal_2\",\"meal_3\",\"meal_4\",\"meal_5\",\"meal_6\",\"meal_7\",\"meal_8\",\"meal_9\",\"meal_10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in top_food_dataframe.columns:\n",
    "    top_food_dataframe[col] = top_food_dataframe[col].apply(lambda food_id: food_names[str(int(food_id))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_food_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> As we can see, Strawberries - Raw is in the top products after January <br> and every month it increases its popularity until May and then the amount of its use decreases again which is logical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#FF7B07\"><div align=\"center\">**Data Cleaning** <a  name=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> Drop applicants which have more than 3 null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = data.index.isin(nullColumns['index']) \n",
    "data.drop(data[cond].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> Drop applicants which have 0 goal calories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = data[data['goal_calories']==0].index\n",
    "zeros = pd.DataFrame(zeros,columns={'index'})\n",
    "\n",
    "cond = data.index.isin(zeros['index']) \n",
    "data.drop(data[cond].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Examples which have zero in goal_calories : ',(len(data[data['goal_calories']==0])/len(data))*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> Delete \"date\" \"food_ids\" and \"sequence\" columns because we no longer use them. all the information we need from them are reflected in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(columns=['date','food_ids','sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> As we mentioned before, there are outliers in our data which needs to be removed because it can mislead our analysis. To perform this, we use standard deviation method for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_std(df, column):\n",
    "    Q1 = df[column].quantile(0.1)\n",
    "    Q3 = df[column].quantile(0.9)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    df = df.query(f'(@Q1 - 1.5 * @IQR) <= {column} <= (@Q3 + 1.5 * @IQR)')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prev_shape = data.shape\n",
    "user_prev_shape = data['user_id'].nunique()\n",
    "\n",
    "for column in data.columns[4:]:\n",
    "    data =  remove_outliers_std(data,column)\n",
    "    \n",
    "print(f'Percenteage of removed outliers in rows point of view {100-data.shape[0]/data_prev_shape[0]*100}')\n",
    "print(f'Percenteage of removed outliers in users point of view {100-data[\"user_id\"].nunique()/user_prev_shape*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> removed rows percentage may appear high, but the point is that in our tasks we should concentrate on users and user's rate is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#FF7B07\"><div align=\"center\">**Create Features** <a  name=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_date_num = 10\n",
    "number_of_last_days = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> As we see one feature which needs to be remaked is food_ids <br> So, we will extract most of the information from the given feature and create a new one <br> so that we can lose as few information as possible.<br> Firstly, we'll create a new attribute based on the size of food_ids . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['foods_len'] = data[\"food_ids\"].apply(lambda x: len(x[1:-1].split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createPlot(data,\"foods_len\",\"Foods len Dist\",\"Foods length\",\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✔️ Foods_len is right skewed (0.8). <br>\n",
    "❌ There are a few outliers in the distribution. <br>\n",
    "✔️ Most of the applicant's foods_len is near 10 <br> \n",
    "❌ Max number of foods in one day is 62 <br>\n",
    "✔️ Min number of foods in one day is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> For each user we may have several rows. <br> Our goal is to transform the data so that we have one characteristic or one row for each person. <br> So let's take a separate feature of how many records we have for each person. <br> It will also help us to store information and according to the algorithm we can select people who for example have less than 10 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> To do this,we create temporary dataframe where we will have unique user IDs and the corresponding number of logs<br>  so that we do not want the values to be repeated and data manipulation to become difficult. <br> Finally after we prepare the data we will add it and we will have all the columns together so do not be confused "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new feature which shows how many days are logged by each applicants\n",
    "user_logged_freq = data[\"user_id\"].value_counts()\n",
    "user_logged_df = pd.DataFrame(data[\"user_id\"].unique(),columns = ['user_id'])\n",
    "user_logged_df[\"logged_frequency\"] = user_logged_df[\"user_id\"].apply(lambda _id: user_logged_freq[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createPlot(user_logged_df,\"logged_frequency\",\"logged Distribution\",\"logged\",\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✔️ logged frequency is right skewed (0.8). <br>\n",
    "✔️ There are no outliers in the distribution. <br>\n",
    "✔️ Most of the logged frequency is near 42 <br> \n",
    "✔️ Max number of logs is 187 <br>\n",
    "❌ Max number of logs is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> It seems that there are people who have 187 records and also people with 1 record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> create another feature based on days and months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_df = pd.DataFrame(data.groupby('user_id').apply(lambda x: datetime.datetime.strptime((x['date'].values[0]), \"%Y-%m-%d\").date().weekday())).reset_index()\n",
    "start_date_df.columns = ['user_id','start_date']\n",
    "user_logged_df = pd.merge(user_logged_df, start_date_df, on=['user_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_logged_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> As we saw, users may have missed data between start and end date, <br> so let's do a separate feature to show that <br> This will help us retain as much information as possible about our customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calls days_missed regarding last and first records\n",
    "def get_missed_days(df,userID,num_days):\n",
    "    head = df[df[\"user_id\"]==userID].head(1)['date'].values[0]\n",
    "    d1 = datetime.datetime.strptime(str(head), \"%Y-%m-%d\").date()\n",
    "    d2 = d1 + datetime.timedelta(num_days)\n",
    "    _df = df[df[\"user_id\"]==userID].head(num_days)\n",
    "    _df[\"in_range\"] = _df['date'].apply(lambda x: d1 <= datetime.datetime.strptime(str(x), \"%Y-%m-%d\").date() <= d2)\n",
    "    return num_days-_df['in_range'].sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new feature based on how many days are missed for each user\n",
    "user_logged_df['days_missed'] = user_logged_df['user_id'].apply(lambda x: get_missed_days(data,x,input_date_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createPlot(user_logged_df,\"days_missed\",\"Missing days Distribution\",\"Missing days\",\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✔️ Missing days is right skewed (0.4). <br>\n",
    "✔️ There are no outliers in the distribution. <br>\n",
    "❌ Most of the Missing frequency is near 48 <br> \n",
    "❌ Max number of logs is 174 <br>\n",
    "✔️ Max number of logs is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> Since the data relates to nutrition, health and exercise, <br>we need to have a separate variable for each row <br> that shows user's nutritions ratio satisfies the known dependence. If it's in accepted norm, we call it healthy distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHealtyDistributedValues(value,lower,upper):\n",
    "    if(value <= lower):\n",
    "        return lower-value # the difference person lacked \n",
    "    if(value >= upper):\n",
    "        return value-upper # the difference person exceed \n",
    "    return 0 # method returns 0 for the values in range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is known that for healthy eating, daily carbs should be between 45-65% out of callories, fats between 10-35% and proteins 20-35%\n",
    "# So this method counts daily norm distribution of person\n",
    "# 0 for the people in persmissible range\n",
    "# max value may be 1.3 (when person only took fats)\n",
    "\n",
    "def healthyDistributed(carbs,fat,protein):\n",
    "    totalCalories = fat*9+ carbs*4 + protein*4 # convert to calories (1g fat = 9 calories and etc..) and sum\n",
    "    sum =  getHealtyDistributedValues ( carbs*4 / (totalCalories+0.00000001),0.45,0.65)\n",
    "    sum += getHealtyDistributedValues ( protein*4 / (totalCalories+0.00000001),0.2,0.35)\n",
    "    sum += getHealtyDistributedValues ( fat*9 / (totalCalories+0.00000001),0.1,0.35)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['healtyDistrib'] = data[['total_carbs','total_fat','total_protein']].apply(lambda x: healthyDistributed(x.total_carbs,x.total_fat,x.total_protein),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createPlot(data,\"healtyDistrib\",\"healthy Distribution\",\"value\",\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✔️ Healthy distribution is right skewed (0.4). <br>\n",
    "❌ There are many outliers in the distribution. <br>\n",
    "✔️ Median is near a range of 0.1 <br> \n",
    "❌ Max number of values is 1.3 <br>\n",
    "✔️ Min number of values is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> One of the algorithms which we'll implement is to predict whether each user approaches the goal in the future or not <br>so we need a variable that describes this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gets all nutrition values with goals and checks if calories difference is less than percentage of goals\n",
    "# TODO:\n",
    "def check_bounds(total_calories, total_carbs, total_fat, total_protein, total_sodium, total_sugar, \n",
    "               goal_calories, goal_carbs, goal_fat, goal_protein, goal_sodium, goal_sugar,percent):\n",
    "    \n",
    "    return (abs(goal_calories - total_calories) < goal_calories * percent / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function checks last days for user and counts number of days when his nutrient was in goal range\n",
    "def reach_goal(df,user_id,num_days):\n",
    "    allowed_difference_percentage = 20\n",
    "    tails = df[df[\"user_id\"]==user_id].tail(num_days)\n",
    "    d2 = datetime.datetime.strptime(tails.iloc[-1]['date'],'%Y-%m-%d').date()\n",
    "    d1 = d2-datetime.timedelta(num_days)\n",
    "    tails = tails[tails['date'].apply(lambda d: d1<datetime.datetime.strptime(d,'%Y-%m-%d').date()<=d2)]\n",
    "    tails[\"reach_goal\"] = tails.apply(lambda row: check_bounds(*(row.values[4:16]),allowed_difference_percentage),axis=1)\n",
    "    return tails[\"reach_goal\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature which shows if user reaches goals in last days \n",
    "# and return 1 if the number of days when user reached goal is greater than threshold else 0\n",
    "# TODO:\n",
    "\n",
    "threshold = 2\n",
    "user_logged_df[\"reach_goal\"] = user_logged_df['user_id'].apply(lambda x: reach_goal(data,x,number_of_last_days))\n",
    "user_logged_df[\"reach_goal\"] = user_logged_df[\"reach_goal\"].apply(lambda x: 1 if x>=threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax  = plt.subplots(1,2,figsize = (18,6))\n",
    "# sns.countplot(x=user_logged_df['reach_goal'],ax=ax[0],palette = 'bright',alpha=0.7)\n",
    "\n",
    "# count = user_logged_df['reach_goal'].value_counts(normalize=True)\n",
    "# count.plot.pie(autopct=\"%.2f%%\",explode = [0,0.2],ax=ax[1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> As we see it, the reach_goal is twice as low, so we will need to treat the imbalance. which we can with two ways: <br> 1. Preproccess data <br> 2. Come up with appropriate metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> Each user has its goal and real received nutrient columns <br> so it is important to know the deviation between them and bring them out as a separate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"calories_diff\"] = data[\"goal_calories\"]-data[\"total_calories\"]\n",
    "data[\"carbs_diff\"] = data[\"goal_carbs\"]-data[\"total_carbs\"]\n",
    "data[\"fat_diff\"] = data[\"goal_fat\"]-data[\"total_fat\"]\n",
    "data[\"protein_diff\"] = data[\"goal_protein\"]-data[\"total_protein\"]\n",
    "data[\"sodium_diff\"] = data[\"goal_sodium\"]-data[\"total_sodium\"]\n",
    "data[\"sugar_diff\"] = data[\"goal_sugar\"]-data[\"total_sugar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([[col,data[col].max()] for col in data.columns[-6:]], columns = ['col','maximum difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods_and_groups.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Food', 'Carbs', 'Dietary Fiber', 'Sugar', 'Fat', 'Saturated', 'Polyunsaturated',\n",
    "        'Monounsaturated', 'Trans','Protein', 'Sodium', 'Potassium', 'Cholesterol',\n",
    "        'Vitamin A', 'Vitamin C', 'Calcium', 'Iron']\n",
    "\n",
    "units = [{'Carbs': 'g'},\n",
    "         {'Dietary Fiber': 'g'},\n",
    "         {'Sugar': 'g'},\n",
    "         {'Fat': 'g'},\n",
    "         {'Saturated': 'g'},\n",
    "         {'Polyunsaturated': 'g'},\n",
    "         {'Monounsaturated': 'g'},\n",
    "         {'Trans': 'g'},\n",
    "         {'Protein': 'g'},\n",
    "         {'Sodium': 'mg'},\n",
    "         {'Potassium': 'mg'},\n",
    "         {'Cholesterol': 'mg'},\n",
    "         {'Vitamin A': '%'},\n",
    "         {'Vitamin C': '%'},\n",
    "         {'Calcium': '%'},\n",
    "         {'Iron': '%'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_food = pd.DataFrame(columns=cols)\n",
    "\n",
    "for food in foods:\n",
    "    values = []\n",
    "    values.append([list(food.keys())[0]])\n",
    "    for info in bs(list(food.values())[0]).findAll(\"div\", {\"class\": \"jss123\"}):\n",
    "        try:\n",
    "            value = int(info.decode_contents().split('<span>')[1].split(' ')[0])\n",
    "        except:\n",
    "            value = [None]\n",
    "        values.append([value])\n",
    "    try:\n",
    "        detailed_food = detailed_food.append(pd.DataFrame(data=np.array(values).T, columns=cols), ignore_index=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "detailed_food.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_food = pd.merge(detailed_food, foods_and_groups, on=['Food'], how='left')\n",
    "detailed_food['Food Group'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_food['Food Group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,16))\n",
    "sns.countplot(y=detailed_food['Food Group'],order=detailed_food['Food Group'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(detailed_food[detailed_food['Food Group'] == 'Spices and Herbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['food_ids'] = data['food_ids'].apply(lambda x: x[1:-1].split(', '))\n",
    "\n",
    "# data is already sorted via date by default. So I just need to take 10 newest row per user \n",
    "nutritional_shares = data[['user_id','total_calories', 'total_carbs', 'total_fat', 'total_protein', \n",
    "                          'total_sodium', 'total_sugar']].groupby(['user_id']).head(input_date_num).groupby(['user_id']).sum().reset_index()\n",
    "\n",
    "nutritional_shares['total_carbs'] = nutritional_shares['total_carbs']/nutritional_shares['total_calories']\n",
    "nutritional_shares['total_fat'] = nutritional_shares['total_fat']/nutritional_shares['total_calories']\n",
    "nutritional_shares['total_protein'] = nutritional_shares['total_protein']/nutritional_shares['total_calories']\n",
    "nutritional_shares['total_sodium'] = nutritional_shares['total_sodium']/nutritional_shares['total_calories']\n",
    "nutritional_shares['total_sugar'] = nutritional_shares['total_sugar']/nutritional_shares['total_calories']\n",
    "nutritional_shares.drop(['total_calories'], axis=1,inplace=True)\n",
    "\n",
    "nutritional_shares.columns = ['user_id', 'Carbs', 'Fat', 'Protein', 'Sodium', 'Sugar']\n",
    "\n",
    "nutritional_shares.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_as_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "detailed_food['Carbs'] = detailed_food['Carbs'].apply(lambda x: cast_as_int(x))\n",
    "detailed_food['Fat'] = detailed_food['Fat'].apply(lambda x: cast_as_int(x))\n",
    "detailed_food['Protein'] = detailed_food['Protein'].apply(lambda x: cast_as_int(x))\n",
    "detailed_food['Sodium'] = detailed_food['Sodium'].apply(lambda x: cast_as_int(x))\n",
    "detailed_food['Sugar'] = detailed_food['Sugar'].apply(lambda x: cast_as_int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_group_share = detailed_food[['Food Group', 'Carbs', 'Fat', 'Protein', 'Sodium', 'Sugar']]\\\n",
    "                                .groupby(['Food Group']).sum().reset_index()\n",
    "\n",
    "food_group_share['Total'] = food_group_share['Carbs']+food_group_share['Fat']+food_group_share['Protein']+\\\n",
    "                            food_group_share['Sodium']+food_group_share['Sugar']\n",
    "\n",
    "food_group_share['Carbs'] = food_group_share['Carbs']/food_group_share['Total']\n",
    "food_group_share['Fat'] = food_group_share['Fat']/food_group_share['Total']\n",
    "food_group_share['Protein'] = food_group_share['Protein']/food_group_share['Total']\n",
    "food_group_share['Sodium'] = food_group_share['Sodium']/food_group_share['Total']\n",
    "food_group_share['Sugar'] = food_group_share['Sugar']/food_group_share['Total']\n",
    "food_group_share.drop(['Total'], axis=1,inplace=True)\n",
    "\n",
    "food_group_share.head(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_user_eating_group(row):\n",
    "    global food_group_share\n",
    "    temp = food_group_share.drop(['Food Group'], axis=1)-row.values\n",
    "    temp['TotalDistance'] = temp['Carbs']**2 + temp['Fat']**2 + temp['Protein']**2 + temp['Sodium']**2 + temp['Sugar']**2\n",
    "    temp['Food Group'] = food_group_share['Food Group']\n",
    "    return temp[temp.TotalDistance == temp.TotalDistance.min()].iloc[0]['Food Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = nutritional_shares.drop(['user_id'], axis=1)\n",
    "FoodGroups = []\n",
    "for i in range(0, len(temp)):\n",
    "    FoodGroups.append(determine_user_eating_group(temp.iloc[i]))\n",
    "    \n",
    "    \n",
    "nutritional_shares['FoodGroup'] = FoodGroups\n",
    "del temp, FoodGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutritional_shares.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['food_ids','sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#FF7B07\"><div align=\"center\">**Data Preparation** <a  name=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> Some of the features had wide range of values so lets scale them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "col_df = data.columns.drop(['user_id','date'])\n",
    "col_user_logged_df = [\"logged_frequency\",\"days_missed\",'start_date']\n",
    "\n",
    "user_logged_df[col_user_logged_df] = scaler.fit_transform(user_logged_df[col_user_logged_df])\n",
    "data[col_df] = scaler.fit_transform(data[col_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> For now, we don't need any columns besides user_id and FoodGroup so drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutritional_shares = nutritional_shares.drop(columns = [\"Carbs\",\"Fat\",\"Protein\",\"Sodium\",\"Sugar\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> FoodGroup is string type so we need to encode them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_dict(df,lst):\n",
    "    \"\"\"\n",
    "    this function creates dictionary for encoding. Its find unique labels for each column and enumerate them\n",
    "    \n",
    "    Arguments:\n",
    "    df -- pandas dataframe\n",
    "    lst -- list of columns which we want to encode \n",
    "    \n",
    "    Returns:\n",
    "    dictionary where key is column name and value is dictionary of unique labels and encoding value\n",
    "    \"\"\"\n",
    "    encoded_dict = {}\n",
    "    for col in lst:\n",
    "        each_dict = {}\n",
    "        sorted_unique_names = df[col].dropna().unique()\n",
    "        sorted_unique_names.sort()\n",
    "        for i,val in enumerate(sorted_unique_names):\n",
    "            each_dict[val] = i\n",
    "        encoded_dict[col] = each_dict\n",
    "    return encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict = get_encoded_dict(nutritional_shares,[\"FoodGroup\"])\n",
    "nutritional_shares = nutritional_shares.replace(encoded_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> As we have seen all users have a different number of records and we want each user to have one characteristic in one row.<br>For this we will take a 5 day record from all users and then we will flatten it. <br> From users who had more than 5 data we will leave the last 5 days. And those who had less than 5 will be filled with -1s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_padding(x,num_row):\n",
    "    # get first row because I need same format and same id, others columns replaced by -1\n",
    "    first_row = x.iloc[0] \n",
    "    first_row[1:] = [-1]*len(first_row[1:])\n",
    "    \n",
    "    d1 = datetime.datetime.strptime(x.iloc[0]['date'],'%Y-%m-%d').date()\n",
    "    padded = []\n",
    "    j = 0\n",
    "    for i in range(num_row):\n",
    "        d2 = d1+datetime.timedelta(i)\n",
    "        try:\n",
    "            if d2 == datetime.datetime.strptime(x.iloc[j]['date'],'%Y-%m-%d').date():\n",
    "                padded.append(x.iloc[j])\n",
    "                j+=1\n",
    "            else:\n",
    "                padded.append(first_row)\n",
    "        except IndexError:\n",
    "            padded.append(first_row)\n",
    "    return pd.DataFrame(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function flattens all rows for each user which we padded already \n",
    "# so creates one vector because we need one input for each user\n",
    "def flatten_rows(x,cols):\n",
    "    global data_temp_df\n",
    "    kk = [x.iloc[0]['user_id']]\n",
    "    x.apply(lambda x: kk.extend(list(x.values[1:])),axis=1)\n",
    "    dictionary = dict(zip(df_columns, kk))\n",
    "    data_temp_df.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may take 5 minutes \n",
    "\n",
    "data = data.groupby('user_id').apply(row_padding,input_date_num).reset_index(drop=True)\n",
    "data_temp_df = []\n",
    "df_columns = list(data.columns.drop('date'))+[str(j)+'_'+str(i) for i in range(1,input_date_num) for j in data.columns[2:]]\n",
    "data = data.drop(columns = ['date'])\n",
    "cols = data.columns\n",
    "data.groupby('user_id').apply(flatten_rows,cols).reset_index(drop=True)\n",
    "data = pd.DataFrame.from_dict(data_temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> merge dataframe with features which we already create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, user_logged_df, on=['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, nutritional_shares, on=['user_id'])\n",
    "data['FoodGroup'] = scaler.fit_transform(np.array(data['FoodGroup']).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#FF7B07\"> Handle missing data with KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer()\n",
    "data[data.columns] = pd.DataFrame(imputer.fit_transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = pd.DataFrame(data.isna().sum())\n",
    "null_df.columns = [\"Null Frequency\"]\n",
    "null_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#00CC00\"> As you see we don't have null values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#FF7B07\"> update dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'../data/myFitnessPal_data_prepared_to_use.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
